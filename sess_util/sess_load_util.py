"""
sess_load_util.py

This module contains functions for loading data from files generated by the 
Allen Institute OpenScope experiments for the Credit Assignment Project.

Authors: Colleen Gillon

Date: August, 2018

Note: this code uses python 3.7.

"""

import copy
import logging
from pathlib import Path

import h5py
import numpy as np
import pandas as pd
import pynwb

from util import file_util, gen_util, logger_util
from sess_util import sess_file_util, sess_gen_util, sess_sync_util

logger = logging.getLogger(__name__)

TAB = "    "

NWB_FILTER_KS = 5

######################################
def get_sessid_from_mouse_df(mouse_n=1, sess_n=1, runtype="prod", 
                             mouse_df="mouse_df.csv"):
    """
    get_sessid_from_mouse_df(sessid)

    Returns session ID, based on the mouse number, session number, and runtype,
    based on the mouse dataframe.

    Optional args:
        - mouse_n (int)  : mouse number
                           default: 1
        - sess_n (int)   : session number
                           default: 1
        - runtype (str)  : type of data
                           default: 1
        - mouse_df (Path): path name of dataframe containing information on each 
                           session. Dataframe should have the following columns:
                               mouse_n, sess_n, runtype
                           default: "mouse_df.csv"

    Returns:
        - sessid (int): session ID
    """

    if isinstance(mouse_df, (str, Path)):
        mouse_df = file_util.loadfile(mouse_df)

    df_line = gen_util.get_df_vals(
        mouse_df, ["mouse_n", "sess_n", "runtype"], 
        [int(mouse_n), int(sess_n), runtype],
        single=True
        )

    sessid = int(df_line["sessid"].tolist()[0])

    return sessid


######################################
def load_info_from_mouse_df(sessid, mouse_df="mouse_df.csv"):
    """
    load_info_from_mouse_df(sessid)

    Returns dictionary containing information from the mouse dataframe.

    Required args:
        - sessid (int): session ID

    Optional args:
        - mouse_df (Path): path name of dataframe containing information on each 
                           session. Dataframe should have the following columns:
                               sessid, mouse_n, depth, plane, line, sess_n, 
                               pass_fail, all_files, any_files, notes
                           default: "mouse_df.csv"

    Returns:
        - df_dict (dict): dictionary with following keys:
            - all_files (bool) : if True, all files have been acquired for
                                 the session
            - any_files (bool) : if True, some files have been acquired for
                                 the session
            - dandi_id (str)   : Dandi session ID
            - date (str)       : session date (i.e., yyyymmdd)
            - depth (int)      : recording depth 
            - plane (str)      : recording plane ("soma" or "dend")
            - line (str)       : mouse line (e.g., "L5-Rbp4")
            - mouse_n (int)    : mouse number (e.g., 1)
            - mouseid (int)    : mouse ID (6 digits)
            - notes (str)      : notes from the dataframe on the session
            - pass_fail (str)  : whether session passed "P" or failed "F" 
                                 quality control
            - runtype (str)    : "prod" (production) or "pilot" data
            - sess_n (int)     : overall session number (e.g., 1)
            - stim_seed (int)  : random seed used to generated stimulus 
    """

    if isinstance(mouse_df, (str, Path)):
        mouse_df = file_util.loadfile(mouse_df)

    df_line = gen_util.get_df_vals(mouse_df, "sessid", sessid, single=True)

    df_dict = {
        "mouse_n"      : int(df_line["mouse_n"].tolist()[0]),
        "dandi_id"     : df_line["dandi_session_id"].tolist()[0],
        "date"         : int(df_line["date"].tolist()[0]),
        "depth"        : df_line["depth"].tolist()[0],
        "plane"        : df_line["plane"].tolist()[0],
        "line"         : df_line["line"].tolist()[0],
        "mouseid"      : int(df_line["mouseid"].tolist()[0]),
        "runtype"      : df_line["runtype"].tolist()[0],
        "sess_n"       : int(df_line["sess_n"].tolist()[0]),
        "stim_seed"    : int(df_line["stim_seed"].tolist()[0]),
        "pass_fail"    : df_line["pass_fail"].tolist()[0],
        "all_files"    : bool(int(df_line["all_files"].tolist()[0])),
        "any_files"    : bool(int(df_line["any_files"].tolist()[0])),
        "notes"        : df_line["notes"].tolist()[0],
    }

    return df_dict


#############################################
def load_small_stim_pkl(stim_pkl, runtype="prod"):
    """
    load_small_stim_pkl(stim_pkl)

    Loads a smaller stimulus dictionary from the stimulus pickle file in which 
    "posbyframe" for visual flow stimuli is not included. 
    
    If it does not exist, small stimulus dictionary is created and saved as a
    pickle with "_small" appended to name.
    
    Reduces the pickle size about 10 fold.

    Required args:
        - stim_pkl (Path): full path name for the full stimulus pickle file
    
    Optional args:
        - runtype (str): runtype ("prod" or "pilot")
    """

    stim_pkl = Path(stim_pkl)
    stim_pkl_no_ext = Path(stim_pkl.parent, stim_pkl.stem)
    small_stim_pkl_name = Path(f"{stim_pkl_no_ext}_small.pkl")
    
    if small_stim_pkl_name.is_file():
        return file_util.loadfile(small_stim_pkl_name)
    else:
        logger.info("Creating smaller stimulus pickle.", extra={"spacing": TAB})

        stim_dict = file_util.loadfile(stim_pkl)

        if runtype == "pilot":
            stim_par_key = "stimParams"
        elif runtype == "prod":
            stim_par_key = "stim_params"
        else:
            gen_util.accepted_values_error(
                "runtype", runtype, ["prod", "pilot"])

        for i in range(len(stim_dict["stimuli"])):
            stim_keys = stim_dict["stimuli"][i][stim_par_key].keys()
            stim_par = stim_dict["stimuli"][i][stim_par_key]
            if runtype == "pilot" and "posByFrame" in stim_keys:
                _ = stim_par.pop("posByFrame")
            elif runtype == "prod" and "square_params" in stim_keys:
                _ = stim_par["session_params"].pop("posbyframe")
                
        file_util.saveinfo(stim_dict, small_stim_pkl_name)

        return stim_dict


#############################################
def load_stim_df_info(stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid, 
                      runtype="prod"):
    """
    load_stim_df_info(stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid)

    Creates the alignment dataframe (stim_df) and saves it as a pickle
    in the session directory, if it does not already exist. Returns dataframe, 
    alignment arrays, and frame rate.
    
    Required args:
        - stim_pkl (Path)    : full path name of the experiment stim pickle 
                               file
        - stim_sync_h5 (Path): full path name of the experiment sync hdf5 file
        - time_sync_h5 (Path): full path name of the time synchronization hdf5 
                               file
        - align_pkl (Path)   : full path name of the output pickle file to 
                               create
        - sessid (int)       : session ID, needed the check whether this 
                               session needs to be treated differently 
                               (e.g., for alignment bugs)

    Optional args:
        - runtype (str): runtype ("prod" or "pilot")
                         default: "prod"

    Returns:
        - stim_df (pd DataFrame): stimlus alignment dataframe with columns:
                                    "stimtype", "unexp", "stim_seg", "gabfr", 
                                    "gab_ori", "gabk", "visflow_dir", 
                                    "visflow_size", "start_twop_fr", 
                                    "end_twop_fr", "num_twop_fr"
        - stimtype_order (list) : stimulus type order
        - stim2twopfr (1D array): 2p frame numbers for each stimulus frame, 
                                  as well as the flanking
                                  blank screen frames 
        - twop_fps (num)        : mean 2p frames per second
        - twop_fr_stim (int)    : number of 2p frames recorded while stim
                                  was playing
    """

    align_pkl = Path(align_pkl)
    sessdir = align_pkl.parent

    # create stim_df if doesn't exist
    if not align_pkl.is_file():
        logger.info(f"Stimulus alignment pickle not found in {sessdir}, and "
            "will be created.", extra={"spacing": TAB})
        sess_sync_util.get_stim_frames(
            stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid, runtype, 
            )
        
    align = file_util.loadfile(align_pkl)

    stim_df = align["stim_df"]
    stim_df = stim_df.rename(
        columns={"GABORFRAME": "gabfr", 
                 "surp": "unexp", # rename surprise to unexpected
                 "stimType": "stimtype",
                 "stimSeg": "stim_seg",
                 "start_frame": "start_twop_fr", 
                 "end_frame": "end_twop_fr", 
                 "num_frames": "num_twop_fr"})
    
    # rename bricks -> visflow
    stim_df["stimtype"] = stim_df["stimtype"].replace({"b": "v"})

    stim_df = modify_visflow_segs(stim_df, runtype)
    stim_df = stim_df.sort_values("start_twop_fr").reset_index(drop=True)

    # note: STIMULI ARE NOT ORDERED IN THE PICKLE
    stimtype_map = {
        "g": "gabors", 
        "v": "visflow"
        }
    stimtype_order = stim_df["stimtype"].map(stimtype_map).unique()
    stimtype_order = list(
        filter(lambda s: s in stimtype_map.values(), stimtype_order))

    # split stimPar1 and stimPar2 into all stimulus parameters
    stim_df["gab_ori"] = stim_df["stimPar1"]
    stim_df["gabk"] = stim_df["stimPar2"]
    stim_df["visflow_size"] = stim_df["stimPar1"]
    stim_df["visflow_dir"] = stim_df["stimPar2"]

    stim_df = stim_df.drop(columns=["stimPar1", "stimPar2"])

    for col in stim_df.columns:
        if "gab" in col:
            stim_df.loc[stim_df["stimtype"] != "g", col] = -1
        if "visflow" in col:
            stim_df.loc[stim_df["stimtype"] != "v", col] = -1

    # expand on direction info
    for direc in ["right", "left"]:
        stim_df.loc[(stim_df["visflow_dir"] == direc), "visflow_dir"] = \
            sess_gen_util.get_visflow_screen_mouse_direc(direc)

    stim2twopfr  = align["stim_align"].astype("int")
    twop_fps     = sess_sync_util.get_frame_rate(stim_sync_h5)[0] 
    twop_fr_stim = int(max(align["stim_align"]))

    return stim_df, stimtype_order, stim2twopfr, twop_fps, twop_fr_stim


#############################################
def load_max_projection_nwb(sess_files):
    """
    load_max_projection_nwb(sess_files)

    Returns maximum projection image of downsampled z-stack as an array, from 
    NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Returns:
        - max_proj (2D array): maximum projection image across downsampled 
                               z-stack (hei x wei), with pixel intensity 
                               in 0 (incl) to 256 (excl) range.
    """

    ophys_file = sess_file_util.select_nwb_sess_path(sess_files, ophys=True)

    with pynwb.NWBHDF5IO(str(ophys_file), "r") as f:
        nwbfile_in = f.read()
        ophys_module = nwbfile_in.get_processing_module("ophys")
        main_field = "PlaneImages"
        data_field = "max_projection"
        try:
            max_proj = ophys_module.get_data_interface(
                main_field).get_image(data_field)
        except KeyError as err:
            raise KeyError(
                "Could not find a maximum projection plane image "
                f"for {ophys_file} due to: {err}"
                )

    return max_proj


#############################################
def load_max_projection(max_proj_png):
    """
    load_max_projection(max_proj_png)

    Returns maximum projection image of downsampled z-stack as an array. 

    Required args:
        - max_proj_png (Path): full path names of the maximum projection png

    Returns:
        - max_proj (2D array): maximum projection image across downsampled 
                               z-stack (hei x wei), with pixel intensity 
                               in 0 (incl) to 256 (excl) range.
    """

    if not Path(max_proj_png).is_file():
        raise OSError(f"{max_proj_png} does not exist.")

    import imageio
    max_proj = imageio.imread(max_proj_png)

    return max_proj


#############################################
def _warn_nans_diff_thr(run, min_consec=5, n_pre_existing=None, sessid=None):
    """
    _warn_nans_diff_thr(run)

    Checks for NaNs in running velocity, and logs a warning about the total 
    number of NaNs, and the consecutive NaNs. Optionally indicates the number 
    of pre-existing NaNs, versus number of NaNs resulting from the difference 
    threshold. 

    Required args:
        - run (1D array): array of running velocities in cm/s

    Optional args:
        - min_consec (num)    : minimum number of consecutive NaN running 
                                values to warn aboout
                                default: 5
        - n_pre_existing (num): number of pre-existing NaNs (before difference 
                                thresholding was used)
                                default: None
        - sessid (int)        : session ID to include in the log or error
                                default: None 
    """

    n_nans = np.sum(np.isnan(run))

    if n_nans == 0:
        return

    split_str = ""
    if n_pre_existing is not None:
        if n_pre_existing == n_nans:
            split_str = " (in pre-processing)"
        elif n_pre_existing == 0:
            split_str = " (using diff thresh)"
        else:
            split_str = (f" ({n_pre_existing} in pre-processing, "
                f"{n_nans - n_pre_existing} more using diff thresh)")

    mask = np.concatenate(([False], np.isnan(run), [False]))
    idx = np.nonzero(mask[1 : ] != mask[ : -1])[0]
    n_consec = np.sort(idx[1 :: 2] - idx[ :: 2])[::-1]

    n_consec_above_min_idx = np.where(n_consec > min_consec)[0]
    
    n_consec_str = ""
    if len(n_consec_above_min_idx) > 0:
        n_consec_str = ", ".join(
            [str(n) for n in n_consec[n_consec_above_min_idx]])
        n_consec_str = (f"\n{TAB}This includes {n_consec_str} consecutive "
            "dropped running values.")

    prop = n_nans / len(run)
    sessstr = "" if sessid is None else f"Session {sessid}: "
    logger.warning(f"{sessstr}{n_nans} dropped running frames "
        f"(~{prop * 100:.1f}%){split_str}.{n_consec_str}", 
        extra={"spacing": TAB})

    return


#############################################
def nan_large_run_differences(run, diff_thr=50, warn_nans=True, 
                              drop_tol=0.0003, sessid=None):
    """
    nan_large_run_differences(run)

    Returns running velocity with outliers replaced with NaNs.

    Required args:
        - run (1D array): array of running velocities in cm/s

    Optional args:
        - diff_thr (int)    : threshold of difference in running velocity to 
                              identify outliers
                              default: 50
        - warn_nans (bool)  : if True, a warning is logged 
                              default: True
        - drop_tol (num)    : the tolerance for proportion running frames 
                              dropped. A warning is produced only if this 
                              condition is not met. 
                              default: 0.0003 
        - sessid (int)      : session ID to include in the log or error
                              default: None 

    Returns:
        - run (1D array): updated array of running velocities in cm/s
    """

    # temorarily remove preexisting NaNs (to be reinserted after)
    original_length = len(run)
    not_nans_idx = np.where(~np.isnan(run))[0]
    run = run[not_nans_idx]
    n_pre_existing = original_length - len(run)

    run_diff = np.diff(run)
    out_idx = np.where((run_diff < -diff_thr) | (run_diff > diff_thr))[0]
    at_idx = -1
    for idx in out_idx:
        if idx > at_idx:
            if idx == 0:
                # in case the first value is completely off
                comp_val = 0
                if np.absolute(run[0]) > diff_thr:
                    run[0] = np.nan
            else:
                comp_val = run[idx]
            while np.absolute(run[idx + 1] - comp_val) > diff_thr:
                run[idx + 1] = np.nan
                idx += 1
            at_idx = idx

    # reinsert pre-existing NaNs
    prev_run = copy.deepcopy(run)
    with gen_util.TempWarningFilter("invalid value", RuntimeWarning):
        run = np.empty(original_length) * np.nan
    run[not_nans_idx] = prev_run

    prop_nans = np.sum(np.isnan(run)) / len(run)
    if warn_nans and prop_nans > drop_tol:
        _warn_nans_diff_thr(
            run, min_consec=5, n_pre_existing=n_pre_existing, sessid=sessid
            )

    return run



#############################################
def load_run_data_nwb(sess_files, diff_thr=50, drop_tol=0.0003, sessid=None):
    """
    load_run_data_nwb(sess_files)

    Returns pre-processed running velocity from NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Optional args:
        - diff_thr (int): threshold of difference in running velocity to 
                          identify outliers
                          default: 50
        - drop_tol (num): the tolerance for proportion running frames 
                          dropped. A warning is produced only if this 
                          condition is not met. 
                          default: 0.0003 
        - sessid (int)  : session ID to include in the log or error
                          default: None 
    Returns:
        - run_velocity (1D array): array of running velocities in cm/s for each 
                                   recorded stimulus frames

    """

    behav_file = sess_file_util.select_nwb_sess_path(sess_files, behav=True)

    with pynwb.NWBHDF5IO(str(behav_file), "r") as f:
        nwbfile_in = f.read()
        behav_module = nwbfile_in.get_processing_module("behavior")
        main_field = "BehavioralTimeSeries"
        data_field = "running_velocity"
        try:
            behav_time_series = behav_module.get_data_interface(
                main_field).get_timeseries(data_field)
        except KeyError as err:
            raise KeyError(
                "Could not find running velocity data in behavioral time "
                f"series for {behav_module} due to: {err}"
                )
        
        run_velocity = np.asarray(behav_time_series.data)

    run_velocity = nan_large_run_differences(
        run_velocity, diff_thr, warn_nans=True, drop_tol=drop_tol, 
        sessid=sessid
        )

    return run_velocity


#############################################
def load_run_data(stim_dict, stim_sync_h5, filter_ks=5, diff_thr=50, 
                  drop_tol=0.0003, sessid=None):
    """
    load_run_data(stim_dict, stim_sync_h5)

    Returns running velocity with outliers replaced with NaNs, and median 
    filters the data.

    Required args:
        - stim_dict (Path or dict): stimulus dictionary or path to dictionary,
                                   containing stimulus information
        - stim_sync_h5 (Path)     : stimulus synchronization file. 

    Optional args:
        - filter_ks (int)   : kernel size to use in median filtering the 
                              running velocity (0 to skip filtering).
                              default: 5
        - diff_thr (int)    : threshold of difference in running velocity to 
                              identify outliers
                              default: 50
        - drop_tol (num)    : the tolerance for proportion running frames 
                              dropped. A warning is produced only if this 
                              condition is not met. 
                              default: 0.0003 
        - sessid (int)      : session ID to include in the log or error
                              default: None 

    Returns:
        - run_velocity (1D array): array of running velocities in cm/s for each 
                                   recorded stimulus frames

    """

    run_kwargs = {
        "stim_sync_h5": stim_sync_h5,
        "filter_ks"   : filter_ks,
    }

    if isinstance(stim_dict, dict):
        run_kwargs["stim_dict"] = stim_dict        
    elif isinstance(stim_dict, (str, Path)):
        run_kwargs["stim_pkl"] = stim_dict
    else:
        raise TypeError(
            "'stim_dict' must be a dictionary or a path to a pickle."
            )

    run_velocity = sess_sync_util.get_run_velocity(**run_kwargs)

    run_velocity = nan_large_run_differences(
        run_velocity, diff_thr, warn_nans=True, drop_tol=drop_tol, 
        sessid=sessid
        )

    return run_velocity


#############################################
def load_pup_data_nwb(sess_files):
    """
    load_pup_data_nwb(sess_files)

    Returns pre-processed pupil data from NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Returns:
        - pup_data_df (pd DataFrame): pupil data dataframe with columns:
            - frames (int)        : frame number
            - pup_diam (float)    : median pupil diameter in pixels
    """

    behav_file = sess_file_util.select_nwb_sess_path(sess_files, behav=True)

    with pynwb.NWBHDF5IO(str(behav_file), "r") as f:
        nwbfile_in = f.read()
        behav_module = nwbfile_in.get_processing_module("behavior")
        main_field = "PupilTracking"
        data_field = "pupil_diameter"
        try:
            behav_time_series = behav_module.get_data_interface(
                main_field).get_timeseries(data_field)
        except KeyError as err:
            raise KeyError(
                "Could not find pupil diameter data in behavioral time "
                f"series for {behav_module} due to: {err}"
                )
        
        pup_data = np.asarray(behav_time_series.data)


    pup_data_df = pd.DataFrame()
    pup_data_df["pup_diam"] = pup_data / sess_sync_util.MM_PER_PIXEL

    pup_data_df.insert(0, "frames", value=range(len(pup_data_df)))

    return pup_data_df  


#############################################
def load_pup_data(pup_data_h5, time_sync_h5):
    """
    load_pup_data(pup_data_h5, time_sync_h5)

    If it exists, loads the pupil tracking data. Extracts pupil diameter
    and position information in pixels, converted to two-photon frames.

    If it doesn't exist or several are found, raises an error.

    Required args:
        - pup_data_h5 (Path or list): path to the pupil data h5 file
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - pup_data (pd DataFrame): pupil data dataframe with columns:
            - frames (int)        : frame number
            - pup_diam (float)    : median pupil diameter in pixels
            - pup_center_x (float): pupil center position for x at 
                                    each pupil frame in pixels
            - pup_center_y (float): pupil center position for y at 
                                    each pupil frame in pixels
    """

    if pup_data_h5 == "none":
        raise OSError("No pupil data file found.")
    elif isinstance(pup_data_h5, list):
        raise OSError("Many pupil data files found.")

    columns = ["nan_diam", "nan_center_x", "nan_center_y"]
    orig_pup_data = pd.read_hdf(pup_data_h5).filter(items=columns).astype(float)
    nan_pup = (lambda name : name.replace("nan_", "pup_") 
        if "nan" in name else name)
    orig_pup_data = orig_pup_data.rename(columns=nan_pup)

    with h5py.File(time_sync_h5, "r") as f:
        twop_timestamps = f["twop_vsync_fall"][:]

        mean_twop_fps = 1.0 / np.mean(np.diff(twop_timestamps))
        delay = int(np.round(mean_twop_fps * 0.1))
        eye_alignment = f["eye_tracking_alignment"][:].astype(int) + delay

    pup_data = pd.DataFrame()
    last_keep = np.where(eye_alignment < len(orig_pup_data))[0][-1]
    for col in orig_pup_data.columns:
        pup_data[col] = orig_pup_data[col].to_numpy()[
            eye_alignment[: last_keep + 1]
            ]

    pup_data.insert(0, "frames", value=range(len(pup_data)))

    return pup_data  


#############################################
def load_pup_sync_h5_data(pup_video_h5):
    """
    load_pup_sync_h5_data(pup_video_h5)

    Returns pupil synchronization information.

    Required args:
        - pup_video_h5 (Path): path to the pupil video h5 file

    Returns:
        - pup_fr_interv (1D array): interval in sec between each pupil 
                                    frame
    """

    with h5py.File(pup_video_h5, "r") as f:
        pup_fr_interv = f["frame_intervals"][()].astype("float64")

    return pup_fr_interv


#############################################
def load_beh_sync_h5_data(time_sync_h5):
    """
    load_beh_sync_h5_data(time_sync_h5)

    Returns behaviour synchronization information.

    Required args:
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - twop2bodyfr (1D array)  : body-tracking video (video-0) frame 
                                    numbers for each 2p frame
        - twop2pupfr (1D array)   : eye-tracking video (video-1) frame 
                                    numbers for each 2p frame
        - stim2twopfr2 (1D array) : 2p frame numbers for each stimulus 
                                    frame, as well as the flanking
                                    blank screen frames (second 
                                    version, very similar to stim2twopfr 
                                    with a few differences)
    """

    with h5py.File(time_sync_h5, "r") as f:
        twop2bodyfr  = f["body_camera_alignment"][()].astype("int")
        twop2pupfr   = f["eye_tracking_alignment"][()].astype("int")
        stim2twopfr2 = f["stimulus_alignment"][()].astype("int")

    return twop2bodyfr, twop2pupfr, stim2twopfr2


#############################################
def load_sync_h5_data(pup_video_h5, time_sync_h5):
    """
    load_sync_h5_data(pup_video_h5, time_sync_h5)

    Returns pupil and behaviour synchronization information.

    Required args:
        - pup_video_h5 (Path): path to the pupil video h5 file
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - pup_fr_interv (1D array): interval in sec between each pupil 
                                    frame
        - twop2bodyfr (1D array)  : body-tracking video (video-0) frame 
                                    numbers for each 2p frame
        - twop2pupfr (1D array)   : eye-tracking video (video-1) frame 
                                    numbers for each 2p frame
        - stim2twopfr2 (1D array) : 2p frame numbers for each stimulus 
                                    frame, as well as the flanking
                                    blank screen frames (second 
                                    version, very similar to stim2twopfr 
                                    with a few differences)
    """

    pup_fr_interv = load_pup_sync_h5_data(pup_video_h5)

    twop2bodyfr, twop2pupfr, stim2twopfr2 = load_beh_sync_h5_data(time_sync_h5)

    return pup_fr_interv, twop2bodyfr, twop2pupfr, stim2twopfr2


#############################################
def modify_visflow_segs(stim_df, runtype="prod"):
    """
    modify_visflow_segs(stim_df)

    Returns stim_df with visual flow segment numbers modified to ensure that
    they are different for the two visual flow stimuli in the production data.

    Required args:
        - stim_df (pd DataFrame): stimlus alignment dataframe with columns:
                                    "stimtype", "unexp", "stim_seg", "gabfr", 
                                    "gab_ori", "gabk", "visflow_dir", 
                                    "visflow_size", "start_twop_fr", 
                                    "end_twop_fr", "num_twop_fr"

    Optional args:
        - runtype (str): runtype
                         default: "prod"

    Returns:
        - stim_df (pd DataFrame): modified dataframe
    """

    if runtype != "prod":
        return stim_df

    stim_df = copy.deepcopy(stim_df)

    visflow_st_fr = gen_util.get_df_vals(
        stim_df, "stimtype", "v", "start_twop_fr", unique=False)
    visflow_num_fr = np.diff(visflow_st_fr)
    num_fr = gen_util.get_df_vals(
        stim_df, "stimtype", "v", "num_twop_fr", unique=False)[:-1]
    break_idx = np.where(num_fr != visflow_num_fr)[0]
    n_br = len(break_idx)
    if n_br != 1:
        raise RuntimeError("Expected only one break in the visual flow "
            f"stimulus, but found {n_br}.")
    
    # last start frame and seg for the first visual flow stim
    last_fr1 = visflow_st_fr[break_idx[0]] 
    last_seg1 = gen_util.get_df_vals(
        stim_df, ["stimtype", "start_twop_fr"], ["v", last_fr1], "stim_seg")[0]
    
    seg_idx = (
        (stim_df["stimtype"] == "v") & 
        (stim_df["start_twop_fr"] > last_fr1)
        )

    new_idx = stim_df.loc[seg_idx]["stim_seg"] + last_seg1 + 1
    stim_df = gen_util.set_df_vals(
        stim_df, seg_idx, "stim_seg", new_idx, in_place=True
        )

    return stim_df


#############################################
def load_sess_stim_seed(stim_dict, runtype="prod"):
    """
    load_sess_stim_seed(stim_dict)

    Returns session's stimulus seed for this session. Expects all stimuli 
    stored in the session's stimulus dictionary to share the same seed.

    Required args:
        - stim_dict (dict): stimlus dictionary

    Optional args:
        - runtype (str): runtype
                         default: "prod"

    Returns:
        - seed (int): session's stimulus seed
    """

    if runtype == "pilot":
        stim_param_key = "stimParams"
        sess_param_key = "subj_params"
    elif runtype == "prod":
        stim_param_key = "stim_params"
        sess_param_key = "session_params"
    else:
        gen_util.accepted_values_error("runtype", runtype, ["pilot", "prod"])

    seeds = []
    for stimulus in stim_dict["stimuli"]:
        seeds.append(stimulus[stim_param_key][sess_param_key]["seed"])
    
    if np.max(seeds) != np.min(seeds):
        raise RuntimeError("Unexpectedly found different seeds for different "
        "stimuli for this session.")
    
    seed = seeds[0]

    return seed

    