"""
logreg.py

This module contains functions to run and analyse logistic regressions 
to predict stimulus information from ROI activity for data generated by the 
AIBS experiments for the Credit Assignment Project

Authors: Colleen Gillon

Date: October, 2018

Note: this code uses python 3.7.

"""

import os
import copy

from joblib import Parallel, delayed
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import pickle as pkl
import torch

from analysis import quint_analys
from util import data_util, file_util, gen_util, logreg_util, math_util, \
                 plot_util
from sess_util import sess_gen_util, sess_ntuple_util, sess_str_util
from plot_fcts import logreg_plots


#############################################
def get_comps(stimtype='gabors', q1v4=False, regvsurp=False):
    """
    get_comps()

    Returns comparisons that fit the criteria.

    Optional args:
        - stimtype (str) : stimtype
                           default: 'gabors'
        - q1v4 (bool)    : if True, analysis is trained on first and tested on 
                           last quintiles
                           default: False
        - regvsurp (bool): if True, analysis is trained on regular and tested 
                           on regular sequences
                           default: False
    
    Returns:
        - comps (list): list of comparisons that fit the criteria
    """

    if stimtype == 'gabors':
        if regvsurp:
            raise ValueError('regvsurp can only be used with bricks.')
        comps = ['surp', 'AvB', 'AvC', 'BvC', 'DvE', 'Aori', 'Bori', 'Cori', 
            'Dori', 'Eori', 'DoriE', 'DoriA', 'BCDoriA', 'BCDoriE', 'ABCoriD', 
            'ABCoriE']
    elif stimtype == 'bricks':
        comps = ['surp', 'dir_all', 'dir_surp', 'dir_reg', 'half_right', 
            'half_left', 'half_diff'] 
        if regvsurp:
            comps = gen_util.remove_if(
                comps, ['surp', 'dir_surp', 'dir_all', 'half_right', 
                'half_left', 'half_diff'])
        if q1v4:
            comps = gen_util.remove_if(
                comps, ['half_left', 'half_right', 'half_diff'])
    else:
        gen_util.accepted_values_error(
            'stimtype', stimtype, ['gabors', 'bricks'])

    return comps


#############################################
def get_class_pars(comp='surp', stimtype='gabors'):
    """
    get_class_pars()
    
    Returns name of the class determining variable, and the surprise values to 
    use for the classes.

    Optional args:
        - comp (str)            : type of comparison
                                  default: 'surp'
        - stimtype (str)        : stimulus type
                                  default: 'gabors'

    Returns:
        - class_var (str)    : variable separating classes (e.g., 'surps', 
                              'gab_ori', 'bri_dir')
        - surps (str or list): surprise values (for each class, if list)
    """

    if stimtype == 'gabors':
        if comp == 'surp':
            class_var = 'surps'
            surps = [0, 1]
        elif comp == 'DvE':
            class_var = 'surps'
            surps = [0, 1]
        elif 'ori' in comp:
            class_var = 'gab_ori'
            gab_letts = [lett.upper() for lett in comp.split('ori') 
                if len(lett) > 0]
            surps = []
            for lett in gab_letts:
                if ('D' in lett) ^ ('E' in lett): # exclusive or
                    surp_val = 1 if 'E' in lett else 0
                    surps.append(surp_val)
                else:
                    surps.append('any')
            if len(gab_letts) == 1:
                surps = surps[0]
            
        elif 'dir' in comp:
            raise ValueError('dir comparison not valid for gabors.')
        else:
            class_var = 'gabfr'
            surps = 'any'

    elif stimtype == 'bricks':
        class_var = 'bri_dir'
        if comp == 'dir_all':
            surps = 'any'
        elif comp == 'dir_reg':
            surps = 0
        elif comp == 'dir_surp':
            surps = 1
        elif comp == 'surp':
            surps = [0, 1]
            class_var = 'surps'
        elif comp in ['half_right', 'half_left', 'half_diff']:
            surps = 'any'
            class_var = comp
        else:
            raise ValueError('Only surp, dir_all, dir_reg, dir_surp, '
                'samehalf, diffhalf comparisons supported for Bricks.')

    return class_var, surps


#############################################
def get_stimpar(comp='surp', stimtype='gabors', bri_dir='both', bri_size=128, 
                gabfr=0, gabk=16, gab_ori='all', bri_pre=0.0):
    """
    get_stimpar()
    
    Returns a stimulus parameter named tuple based on the stimulus parameters 
    passed and comparison type.

    Optional args:
        - comp (str)            : type of comparison
                                  default: 'surp'
        - stimtype (str)        : stimulus type
                                  default: 'gabors'
        - bri_dir (str or list) : brick direction
                                  default: 'both'
        - bri_size (int or list): brick direction
                                  default: 128
        - gabfr (int or list)   : gabor frame of reference (may be a list 
                                  depending on 'comp')
                                  default: 0
        - gabk (int or list)    : gabor kappa
                                  default: 16
        - gab_ori (str)         : gabor orientations ('all' or 'shared'), 
                                  for comp values like DoriE, DoriA, etc.
                                  default: 'all'
        - bri_pre (int)         : pre parameter for Bricks
                                  default: 0.0 

    Returns:
        - stimpar (StimPar)  : named tuple containing stimulus parameters
    """

    if stimtype == 'bricks' and 'half' in bri_dir or 'dir' in bri_dir:
        print('Ignoring brick dir setting.')

    if not (len(comp.replace('ori', '').upper()) > 1):
        gab_ori = 'all'

    [bri_dir, bri_size, gabfr, 
        gabk, gab_ori] = sess_gen_util.get_params(
            stimtype, bri_dir, bri_size, gabfr, gabk, gab_ori)

    if stimtype == 'gabors':
        # DO NOT ALLOW OVERLAPPING
        if comp == 'surp':
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, bri_dir,  bri_size, gabfr, gabk, gab_ori, 0, 1.5)
        elif comp == 'DvE':
            gabfr   = sess_str_util.gabfr_nbrs(comp[0])
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, bri_dir, bri_size, gabfr, gabk, gab_ori, 0, 0.45)
        elif 'ori' in comp:
            gab_letts = [lett.upper() for lett in comp.split('ori')
                if len(lett) > 0]
            act_gabfr = [[sess_str_util.gabfr_nbrs(lett) for lett in letts] 
                for letts in gab_letts]
            pre, post = 0, 0.45
            if len(act_gabfr) == 1:
                act_gabfr = act_gabfr[0]
                if act_gabfr != gabfr:
                    print(f'Setting gabfr to {act_gabfr} instead of {gabfr}.')
            else:
                pre, post = -0.15, 0.45
                gab_ori = sess_gen_util.gab_oris_shared_E(gab_letts, gab_ori)
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, bri_dir, bri_size, act_gabfr, gabk, gab_ori, 
                pre, post)         
        elif 'dir' in comp or 'half' in comp:
            raise ValueError('dir/half comparison not valid for gabors.')
        else:
            gabfrs = sess_str_util.gabfr_nbrs([comp[0], comp[2]])
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, bri_dir, bri_size, gabfrs, gabk, gab_ori, 0, 0.45)
    elif stimtype == 'bricks':
        # DO NOT ALLOW OVERLAPPING
        if 'right' in comp:
            bri_dir = 'right'
        elif 'left' in comp:
            bri_dir = 'left'
        stimpar = sess_ntuple_util.init_stimpar(
            stimtype, bri_dir, bri_size, gabfr, gabk, gab_ori, bri_pre, 1.0)

    return stimpar


#############################################
def get_rundir(run_val, uniqueid=None, alg='sklearn'):
    """
    get_rundir(run_val)

    Returns the name of the specific subdirectory in which an analysis is
    saved, based on a run number and unique ID.
    
    Required args:
        - run_val (int): run number ('pytorch' alg) or 
                         number of run ('sklearn' alg)
    
    Optional args:
        - uniqueid (str or int): unique ID for analysis
                                 default: None
        - alg (str)            : algorithm used to run logistic regression 
                                 ('sklearn' or 'pytorch')
                                 default: 'sklearn'

    Returns:
        - rundir (str): name of subdirectory to save analysis in
    """


    if uniqueid is None:
        if alg == 'sklearn':
            rundir = f'{run_val}_runs'
        elif alg == 'pytorch':
            rundir = f'run_{run_val}'
        else:
            gen_util.accepted_values_error('alg', alg, ['sklearn', 'pytorch'])
    else:
        rundir = f'{uniqueid}_{run_val}'

    return rundir


#############################################
def get_compdir_dict(rundir, no_lists=False):
    """
    get_compdir_dict(rundir)

    Returns a dictionary with analysis parameters based on the full analysis 
    path.
    
    Required args:
        - rundir (str): path of subdirectory in which analysis is saved,
                        structured as 
                        '.../m_s_plane_stim_fluor_scaled_comp_shuffled/
                        uniqueid_run'
    
    Optional args:
        - no_lists (bool): if True, list parameters are replaced with a string, 
                           e.g. 'both'
                           False
    
    Returns:
        - compdir_dict (dict): parameter dictionary
            - bri_dir (str or list) : Bricks direction parameter ('right', 
                                      'left', ['right', 'left'] or 'none') 
            - bri_size (int or list): Bricks size parameter (128, 256, 
                                      [128, 256] or 'none')
            - comp (str)            : comparison parameter ('surp', 'AvB',
                                      'AvC', 'BvC' or 'DvE', None)
            - fluor (str)           : fluorescence parameter ('raw' or 'dff')
            - gabk (int or list)    : Gabor kappa parameter (4, 16, [4, 16] or 
                                      'none')
            - plane (str)           : plane ('soma' or 'dend')
            - mouse_n (int)         : mouse number
            - sess_n (int)          : session number
            - scale (bool)          : scaling parameter
            - run_n (int)           : run number
            - shuffle (bool)        : shuffle parameter
            - stimtype (str)        : stimulus type ('gabors' or 'bricks')
            - uniqueid (str)        : unique ID (datetime, 6 digit number or 
                                      None)
    """

    parts    = rundir.split(os.sep)
    param_str = parts[-2]
    run_str   = parts[-1]

    compdir_dict = sess_gen_util.get_params_from_str(param_str, no_lists)

    if 'run' in run_str:
        compdir_dict['uniqueid'] = None
        compdir_dict['run_n']    = int(run_str.split('_')[1])
    else:
        compdir_dict['uniqueid'] = '_'.join(
            [str(sub) for sub in run_str.split('_')[:-1]])
        compdir_dict['run_n']    = int(run_str.split('_')[-1])    

    return compdir_dict


#############################################
def get_df_name(task='analyse', stimtype='gabors', comp='surp', ctrl=False, 
                alg='sklearn'):
    """
    get_df_name()

    Returns a dictionary with analysis parameters based on the full analysis 
    path.
    
    Optional args:
        - task (str)    : type of task for which to get the dataframe 
                          default: 'analyse'
        - stimtype (str): type of stimulus
                          default: 'gabors'
        - comp (str)    : type of comparison
                          default: 'surp'
        - ctrl (bool)   : if True, control comparisons are analysed
                          default: False
        - alg (str)     : algorithm used to run logistic regression 
                          ('sklearn' or 'pytorch')
                          default: 'sklearn'

    Returns:
        - df_name (str): name of the dataframe
    """

    alg_str = ''
    if alg == 'pytorch':
        alg_str = '_pt'
    elif alg != 'sklearn':
        gen_util.accepted_values_error('alg', alg, ['pytorch', 'sklearn'])

    ctrl_str = sess_str_util.ctrl_par_str(ctrl)

    sub_str = f'{stimtype[0:3]}_{comp}{ctrl_str}{alg_str}'

    if task == 'collate':
        df_name = f'{sub_str}_all_scores_df.csv'
    elif task == 'analyse':
        df_name = f'{sub_str}_score_stats_df.csv'
    
    return df_name


#############################################
def info_dict(analyspar=None, sesspar=None, stimpar=None, extrapar=None, 
              comp='surp', alg='sklearn', n_rois=None, epoch_n=None):
    """
    info_dict()

    Returns an info dictionary from the parameters. Includes epoch number if it 
    is passed. 

    Returns an ordered list of keys instead if any of the dictionaries or
    namedtuples are None.
    
    Required args:
        - analyspar (AnalysPar): named tuple containing analysis parameters
                                 default: None
        - sesspar (SessPar)    : named tuple containing session parameters
                                 default: None
        - stimpar (StimPar)    : named tuple containing stimulus parameters
                                 default: None
        - extrapar (dict)      : dictionary with extra parameters
                                 default: None
            ['run_n'] (int)   : run number
            ['shuffle'] (bool): whether data is shuffled
            ['uniqueid'] (str): uniqueid

    Optional args:
        - comp (str)   : comparison type
                         default: 'surp'
        - alg (str)    : algorithm used to run logistic regression 
                         ('sklearn' or 'pytorch')
                         default: 'sklearn'
        - n_rois (int) : number of ROIs
                         default: None
        - epoch_n (int): epoch number
                         default: None
    
    Returns:
        if all namedtuples and dictionaries are passed:
            - info (dict): analysis dictionary
        else if any are None:
            - info (list): list of dictionary keys
    """

    if not any(par is None for par in [analyspar, sesspar, stimpar, extrapar]):
        if stimpar.stimtype == 'bricks':
            bri_dir = gen_util.list_if_not(stimpar.bri_dir)
            if len(bri_dir) == 2:
                bri_dir = 'both'
            else:
                bri_dir = bri_dir[0]
        else:
            bri_dir = stimpar.bri_dir

        info = {'mouse_n' : sesspar.mouse_n,
                'sess_n'  : sesspar.sess_n,
                'plane'   : sesspar.plane,
                'line'    : sesspar.line,
                'fluor'   : analyspar.fluor,
                'scale'   : analyspar.scale,
                'shuffle' : extrapar['shuffle'],
                'stimtype': stimpar.stimtype,
                'bri_dir' : bri_dir,
                'comp'    : comp,
                'uniqueid': extrapar['uniqueid'],
                'runtype' : sesspar.runtype,
                'n_rois'  : n_rois
                }

        if alg == 'pytorch':
            info['run_n'] = extrapar['run_n']

        if epoch_n is not None:
            info['epoch_n'] = epoch_n

    # if no args are passed, just returns keys
    else:
        info = ['mouse_n', 'sess_n', 'plane', 'line', 'fluor', 'scale', 
            'shuffle', 'stimtype', 'bri_dir', 'comp', 'uniqueid', 'run_n', 
            'runtype', 'n_rois', 'epoch_n']
    return info


#############################################
def save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar): 
    """
    save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    Saves the hyperparameters for an analysis.
    
    Required args:
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ['dirname'] (str): directory in which to save hyperparameters
    
    Returns:
        - hyperpars (dict): hyperparameter dictionary with inputs as keys and 
                            named tuples converted to dictionaries
    """

    hyperpars = {'analyspar': analyspar._asdict(),
                 'logregpar': logregpar._asdict(),
                 'sesspar'  : sesspar._asdict(),
                 'stimpar'  : stimpar._asdict(),
                 'extrapar' : extrapar
                }

    file_util.saveinfo(hyperpars, 'hyperparameters.json', extrapar['dirname'])

    return hyperpars


#############################################
def get_classes(comp='surp', gab_ori='shared'):
    """
    get_classes()

    Returns names for classes based on the comparison type.
    
    Optional args:
        - comp (str)           : type of comparison
                                 default: 'surp'
        - gab_ori (str or list): Gabor orientations
                                 default: 'all'
    Returns:
        - classes (list): list of class names
    """

    if gab_ori == 'all':
        gab_ori = [0, 45, 90, 135]

    if comp == 'surp':
        classes = ['Regular', 'Surprise']
    elif comp in ['AvB', 'AvC', 'BvC', 'DvE']:
        classes = [f'Gabor {fr}' for fr in [comp[0], comp[2]]]    
    elif 'ori' in comp:
        deg_vals = gab_ori
        stripped = comp.replace('ori', '')
        if stripped == 'E':
            deg_vals = [val + 90 for val in deg_vals]
        elif len(stripped) == 2:
            deg_vals = gab_ori[0]
        deg = u'\u00B0'
        classes = [f'{val}{deg}' for val in deg_vals]
    elif 'dir' in comp :
        classes = ['Right', 'Left']
    elif 'half' in comp:
        classes = ['First half', 'Second half']
    else:
        gen_util.accepted_values_error('comp', comp, 
            ['surp', 'AvB', 'AvC', 'BvC', 'DvE', 'dir...', '...ori...'])

    return classes


#############################################
def get_data(stim, analyspar, stimpar, quintpar, qu_i=0, surp=[0, 1], 
             n=1, remconsec_surps=False, get_2nd=False):
    """
    get_data(sess, quintpar, stimpar)

    Returns ROI data based on specified criteria. 

    Required args:
        - stim (Stim)          : stimulus object
        - analyspar (AnalysPar): named tuple containing analysis parameters        
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - quintpar (QuintPar)  : named tuple containing quintile parameters
    
    Optional args:
        - qu_i (int)            : quintile index
                                  default: 0
        - surp (list)           : surprise values
                                  default: [0, 1]
        - n (int)               : factor by which to multiply number of 
                                  surprise values
                                  default: 1
        - remconsec_surps (bool): whether consecutive segments are removed for 
                                  surprise segments
                                  default: False
        - get_2nd (bool)        : if True, every second segment is retained
                                  default: False

    Returns:
        - roi_data (3D array): ROI data, as sequences x frames x ROIs
        - surp_n (int)       : Number of surprise sequences
    """
   
    # data for single quintile
    # first number of surprises, then segs
    for t, surp_use in enumerate([1, surp]):
        remconsec = (remconsec_surps and surp_use == 1)
        segs = quint_analys.quint_segs(
            stim, stimpar, quintpar.n_quints, qu_i, surp_use, 
            remconsec=remconsec)[0][0]
        # get alternating for consecutive segments
        if get_2nd and not remconsec: 
            segs = gen_util.get_alternating_consec(segs, first=False)
        if t == 0:
            surp_n = len(segs) * n
    twop_fr = stim.get_twop_fr_by_seg(segs, first=True)['first_twop_fr']
    
    # do not scale (scaling factors cannot be based on test data)
    roi_data = gen_util.reshape_df_data(
        stim.get_roi_data(twop_fr, stimpar.pre, stimpar.post, 
        analyspar.fluor, remnans=True, scale=False), squeeze_cols=True)
    
    # if remconsec_surps:
        # Normalize to first half
        # mid = roi_data.shape[-1]//2
        # div = np.median(roi_data[:, :, :mid], axis=-1)
        # roi_data = roi_data - np.expand_dims(div, -1)
        
        # Mean and std
        # roi_data = np.stack([np.nanmean(roi_data, axis=0),
        #     np.nanstd(roi_data, axis=0)], axis=0)
        
        # Mean only
        # roi_data = np.expand_dims(np.nanmean(roi_data, axis=0), axis=0)
    
    # transpose to seqs x frames x ROIs
    roi_data = np.transpose(roi_data, [1, 2, 0])

    return roi_data, surp_n


#############################################
def get_sess_data(sess, analyspar, stimpar, quintpar, class_var='surps', 
                  surps=[0, 1], regvsurp=False, split_oris=False):
    """
    get_sess_data(sess, analyspar, stimpar, quintpar)

    Print session information and returns ROI trace segments, target classes 
    and class information and number of surprise segments in the dataset.
    
    Required args:
        - sess (Session)       : session
        - analyspar (AnalysPar): named tuple containing analysis parameters        
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - quintpar (QuintPar)  : named tuple containing quintile parameters

    Optional args:
        - class_var (str)          : class determining variable ('surps' or 
                                     stimpar attribute)
                                     default: 'surps'
        - surps (list, str, int)   : surprise value(s) (list if class_var is 
                                     'surps', otherwise 0, 1 or 'any')
        - regvsurp (bool)          : if True, the first dataset will include 
                                     regular sequences and the second will 
                                     include surprise sequences
                                     default: False
        - split_oris (bool or list): List of Gabor frames for each split, or 
                                     False if splitting orientation comparison 
                                     is not applicable.
                                     default: False
    Returns:
        - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs
                              (1 or 2 if an additional test set is included), 
                              each structured as sequences x frames x ROIs
        - seq_classes (list): list of 2D arrays of sequence classes
                              (1 or 2 if an additional test set is included), 
                              each structured as class values x 1
        - n_surps (list)    : list of lists of number of surprise sequences
                              (doubled if 'half' comparison), 
                              structured as datasets x class 
    """

    stim = sess.get_stim(stimpar.stimtype)

    split_oris = split_oris is not False # set to boolean

    if (regvsurp + (len(quintpar.qu_idx) > 1) + ('half' in class_var)
        + split_oris) > 1:
        raise ValueError('Cannot combine any of the following: separating '
            'quintiles, regvsurp, half comparisons, multiple Gabor frame '
            'orientation comparisons.')
    elif len(quintpar.qu_idx) > 2:
        raise ValueError('Max of 2 quintiles expected.')
    elif split_oris and len(stimpar.gabfr) > 2:
        raise ValueError('Max of 2 Gabor frame sets expected for orientation '
            'classification.')

    # check for stimulus pre/post problems
    pre_post_err = False
    get_2nd, remconsec_surps = False, False
    if stimpar.pre > 0:
        if stimpar.stimtype == 'bricks' and stimpar.pre == 1:
            get_2nd = True
            remconsec_surps = class_var == 'surps'
        else:
            pre_post_err = True
    if stimpar.post > 1.0:
        if not stimpar.stimtype == 'gabors' and stimpar.post <= 1.5:
            pre_post_err = True
    if pre_post_err:
        raise NotImplementedError('Not implemented to prevent sequence overlap '
            f'for {stimpar.stimtype}: {stimpar.pre} pre/{stimpar.post} post')

    n = 1
    if class_var == 'surps':
        n_cl = len(surps)
    elif 'half' in class_var:
        n_cl = 2
        # DOUBLE surp ns to compensate for shorter blocks, if using control
        n = 2
        if 'diff' in class_var:
            quintpar = sess_ntuple_util.init_quintpar(
                4, [[1, 2]], [None], [None])
            if len(np.unique(stim.direcs)) != 2:
                raise ValueError(
                    'Segments do not fit these criteria (missing directions).')
        else:
            quintpar = sess_ntuple_util.init_quintpar(
                2, [[0, 1]], [None], [None])
    else:
        n_cl = len(stimpar._asdict()[class_var])

    # modify surps, qu_idx, gabfr to cycle through datasets
    if len(quintpar.qu_idx) == 2:
        surps = [surps, surps]
        gabfr_idxs = ['ignore', 'ignore']
        if regvsurp:
            raise ValueError(
                'Cannot set regvsurp to True if more than 1 quintile.')
        if 'part' in class_var:
            raise ValueError('Cannot do half comparisons with quintiles.')
    elif regvsurp:
        surps = [surps, 1-surps]
        gabfr_idxs = ['ignore', 'ignore']
        quintpar = sess_ntuple_util.init_quintpar(
            1, [0, 0], [None, None], [None, None])
    elif split_oris:
        surps = surps
        gabfr_idxs = [0, 1]
        quintpar = sess_ntuple_util.init_quintpar(
            1, [0, 0], [None, None], [None, None])
    else:
        surps = [surps]
        gabfr_idxs = ['ignore']
    gabfr_idxs = [0, 1] if split_oris else ['ignore', 'ignore']

    # cycle through classes
    roi_seqs    = [[] for _ in range(len(quintpar.qu_idx))]
    seq_classes = [[] for _ in range(len(quintpar.qu_idx))]
    surp_ns     = [[] for _ in range(len(quintpar.qu_idx))]

    # cycle through data groups (quint or regvsurp or gabfr for oris)        
    for d, (qu_i, subsurps, gabfr_idx) in enumerate(
        zip(quintpar.qu_idx, surps, gabfr_idxs)):
        for cl in range(n_cl):
            use_qu_i = [qu_i]
            surp = subsurps
            stimpar_sp = stimpar
            if class_var == 'surps':
                surp = subsurps[cl]
            elif 'half' in class_var:
                use_qu_i = [qu_i[cl]]
            else:
                keys = class_var
                vals = stimpar._asdict()[class_var][cl]
                if split_oris:
                    keys = [keys, 'gabfr', 'gab_ori']
                    vals = [vals, stimpar.gabfr[gabfr_idx], 
                        stimpar.gab_ori[gabfr_idx][cl]]
                # modify stimpar
                stimpar_sp = sess_ntuple_util.get_modif_ntuple(
                    stimpar, keys, vals)

            roi_data, surp_n = get_data(
                stim, analyspar, stimpar_sp, quintpar, qu_i=use_qu_i, 
                surp=surp, remconsec_surps=remconsec_surps, n=n,  
                get_2nd=get_2nd)

            roi_seqs[d].append(roi_data)
            seq_classes[d].append(np.full(len(roi_data), cl))
            surp_ns[d].append(surp_n)

        # concatenate data split by class along trial seqs axis
        roi_seqs[d] = np.concatenate(roi_seqs[d], axis=0)
        seq_classes[d] = np.concatenate(seq_classes[d], axis=0)
        
    # get logistic variance across datasets
    log_var = np.log(np.var(np.concatenate(roi_seqs, axis=0)))
    n_fr, nrois = roi_seqs[0].shape[1:] # in training set

    if stimpar.stimtype == 'gabors':
        surp_use = surps[0]
        if surp_use == [0, 1] and not isinstance(stimpar.gabfr, list):
            surp_use = 'any'
        if split_oris:
            gabfr_lett = [sess_str_util.gabfr_letters(
                gabfr, surp=surp_use) for gabfr in stimpar.gabfr]
            gabfr_lett = ' -> '.join([str(lett) for lett in gabfr_lett])
        else:
            gabfr_lett = sess_str_util.gabfr_letters(
                stimpar.gabfr, surp=surp_use)
        stim_info = f'\nGab fr: {gabfr_lett}\nGab K: {stimpar.gabk}'
    elif stimpar.stimtype == 'bricks':
        stim_info = (f'\nBri dir: {stimpar.bri_dir}\n'
            f'Bri size: {stimpar.bri_size}')

    print(f'\nRuntype: {sess.runtype}\nMouse: {sess.mouse_n}\n'
        f'Sess: {sess.sess_n}\nPlane: {sess.plane}\nLine: {sess.line}\n'
        f'Fluor: {analyspar.fluor}\nROIs: {nrois}{stim_info}\n'
        f'Frames per seg: {n_fr}\nLogvar: {log_var:.2f}')

    return roi_seqs, seq_classes, surp_ns


#############################################
def sample_seqs(roi_seqs, seq_classes, n_surp):
    """
    sample_seqs(roi_seqs, seq_classes, n_surp)

    Samples sequences to correspond to the ratio of surprise to regular 
    sequences.
    
    Required args:
        - roi_seqs (3D array)   : array of all ROI trace sequences, structured 
                                  as: sequences x frames x ROIs
        - seq_classes (2D array): array of all sequence classes (0, 1), 
                                  structured as class values x 1
        - n_surp (int)          : number of surprise sequences

    Returns:
        - roi_seqs (3D array)   : array of selected ROI trace sequences, 
                                  structured as sequences x frames x ROIs
        - seq_classes (2D array): array of sequence classes, structured as 
                                  class values x 1
    """
    if np.unique(seq_classes).tolist() != [0, 1]:
        raise ValueError('Function expects classes 0 and 1 only.')

    class0_all = np.where(seq_classes == 0)[0]
    class1_all = np.where(seq_classes == 1)[0]
    n_reg = (len(class0_all) + len(class1_all))//2 - n_surp

    class0_idx = np.random.choice(class0_all, n_reg, replace=False)
    class1_idx = np.random.choice(class1_all, n_surp, replace=False)
    
    roi_seqs = np.concatenate(
        [roi_seqs[class0_idx], roi_seqs[class1_idx]], axis=0)

    seq_classes = np.concatenate(
        [seq_classes[class0_idx], seq_classes[class1_idx]], axis=0)
    return roi_seqs, seq_classes


#############################################
def save_tr_stats(plot_data, plot_targ, data_names, analyspar, stimpar, n_rois, 
                  alg='sklearn',mod=None, dirname='.'):
    """
    save_tr_stats(plot_data, plot_targ, data_names, stimpar, n_rois)

    Extracts, saves and returns trace statistics in a json in the specified 
    directory.

    Required args:
        - plot_data (list)     : list of 3D arrays of selected ROI trace seqs 
                                 to be plotted, each structured as 
                                      sequences x frames x ROIs
        - plot_targ (list)     : list of 2D arrays of sequence classes to be 
                                 plotted, each structured as class values x 1
        - data_names (list)    : names for each plot_data array
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (SessPar)    : named tuple containing stimulus parameters
        - n_rois (int)         : number of ROIs in data
    
    Optional args:
        - alg (str)             : algorithm used to run logistic regression 
                                  ('sklearn' or 'pytorch')
                                  default: 'sklearn'
        - mod (sklearn pipeline): sklearn pipeline (model). Required if alg is 
                                  'sklearn'
                                  default: None
        - dirname (str)         : directory in which to save the traces
                                  default: '.'

    Returns:
        - tr_stats (dict)           : dictionary of trace stats data
            ['n_rois'] (int)                  : number of ROIs
            ['train_ns'] (list)               : number of segments per class
            ['train_class_stats'] (3D array)  : training statistics, structured
                                                as class x stats (me, err) x 
                                                   frames
            ['xran'] (array-like)             : x values for frames
            
            optionally, if an additional named set (e.g., 'test_Q4') is passed:
            ['set_ns'] (list)             : number of segments per class
            ['set_class_stats'] (3D array): trace statistics, 
                                                  structured as 
                                                  class x stats (me, err) x 
                                                  frames
    """

    if len(data_names) != len(plot_data):
        raise ValueError('Not as many `plot_data` items as `data_names`.')

    tr_stats = {'n_rois': n_rois}
    classes = np.unique(plot_targ[0])

    for data, targ, name in zip(plot_data, plot_targ, data_names): # get stats
        if data is None:
            continue
        if alg == 'sklearn':
            # scales, flattens and optionally shuffles data
            data = logreg_util.get_transf_data_sk(mod, data, False, 
                name=='train')
        elif alg == 'pytorch':
            data = data.numpy()
            targ = targ.numpy()
        else:
            gen_util.accepted_values_error('alg', alg, ['sklearn', 'pytorch'])
        xran, class_stats, ns = logreg_util.get_stats(
            data, targ, stimpar.pre, stimpar.post, classes, analyspar.stats, 
            analyspar.error)
        tr_stats['xran'] = xran.tolist()
        tr_stats[f'{name}_class_stats'] = class_stats.tolist()
        tr_stats[f'{name}_ns'] = ns

    file_util.saveinfo(tr_stats, 'tr_stats.json', dirname)

    return tr_stats


#############################################
@logreg_util.catch_set_problem_decorator
def init_logreg_model_pt(roi_seqs, seq_classes, logregpar, extrapar, 
                         scale=True, device='cpu', thresh_cl=2):
    """
    init_logreg_model_pt(roi_seqs, seq_classes, logregpar, extrapar)

    Initializes and returns the pytorch logreg model and dataloaders.
    
    Required args:
        - roi_seqs (list)       : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
        - seq_classes (list)    : list of 2D arrays of sequence classes (0 or 1)
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
        - logregpar (LogRegPar) : named tuple containing logistic regression 
                                  parameters
        - extrapar (dict)       : dictionary with extra parameters
            ['shuffle'] (bool): if True, data is shuffled
    
    Optional args:
        - scale (bool)          : whether data is scaled by ROI
                                  default: True
        - device (str)          : device to use
                                  default: 'cpu'
        - thresh_cl (int)       : size threshold for classes in each non empty 
                                  set beneath which the indices are reselected 
                                  (only if targets are passed). Not checked if 
                                  thresh_cl is 0.
                                  default: 2

    Returns:
        - model (torch.nn.Module)        : Neural network module with optimizer 
                                           and loss function as attributes
        - dls (list of torch DataLoaders): list of torch DataLoaders for 
                                           each set. If a set is empty, the 
                                           corresponding dls value is None.
        - extrapar (dict)                : dictionary with extra parameters
            ['cl_wei'] (list)      : list of weights for each class
            ['loss_name'] (str)    : name of the loss function used
            ['sc_facts'] (list)    : min/max value(s) with which to scale
            ['shuffle'] (bool)     : if True, data is shuffled
            ['shuff_reidx'] (list) : list of indices with which targets were 
                                     shuffled 
    """

    sc_dim = 'last' if scale else 'none'

    if np.unique(seq_classes[0]).tolist() != [0, 1]:
        raise NotImplementedError('This Pytorch logreg function is '
            'implemented only for classes 0 and 1.')

    if len(roi_seqs) > 2:
        raise ValueError('Must pass no more than 2 sets of data, but '
            f'found {len(roi_seqs)}.')

    dl_info = data_util.create_dls(
        roi_seqs[0], seq_classes[0], train_p=logregpar.train_p, sc_dim=sc_dim, 
        sc_type='stand_rob', extrem='perc', shuffle=extrapar['shuffle'], 
        batchsize=logregpar.batchsize, thresh_cl=thresh_cl)
    dls = dl_info[0]

    extrapar = copy.deepcopy(extrapar)

    if scale:
       extrapar['sc_facts'] = dl_info[-1]
    
    if len(roi_seqs) == 2:
        class_vals = seq_classes[1]
        if extrapar['shuffle']:
            np.random.shuffle(class_vals)
        if scale:
            roi_seqs[1] = data_util.scale_datasets(
                torch.Tensor(roi_seqs[1]), sc_facts=extrapar['sc_facts'])[0]
        dls.append(data_util.init_dl(
            roi_seqs[1], class_vals, logregpar.batchsize))

    if extrapar['shuffle']:
        extrapar['shuff_reidx'] = dl_info[1]
 
    # from train targets
    extrapar['cl_wei'] = logreg_util.class_weights(dls[0].dataset.targets) 

    n_fr, n_rois  = roi_seqs[0].shape[1:]
    model         = logreg_util.LogReg(n_rois, n_fr).to(device)
    model.opt     = torch.optim.Adam(
        model.parameters(), lr=logregpar.lr, weight_decay=logregpar.wd)
    model.loss_fn = logreg_util.weighted_BCE(extrapar['cl_wei'])
    extrapar['loss_name'] = model.loss_fn.name
    
    return model, dls, extrapar


#############################################
def save_scores(info, scores, key_order=None, dirname=''):
    """
    save_scores(args, scores, saved_eps)

    Saves run information and scores per epoch as a dataframe.
    
    Required args:
        - info (dict)          : dictionary of parameters (see info_dict())
        - scores (pd DataFrame): dataframe of recorded scores
    
    Optional args:
        - key_order (list): ordered list of keys
                            default: None
        - dirname (str)   : directory in which to save scores
                            default: ''
    
    Returns:
        - summ_df (pd DataFrame): dataframe with scores and recorded parameters
    """

    summ_df = copy.deepcopy(scores)

    if key_order is None:
        key_order = info.keys()

    for key in reversed(key_order):
        if key in info.keys():
            summ_df.insert(0, key, info[key])

    file_util.saveinfo(summ_df, 'scores_df.csv', dirname)

    return summ_df


#############################################
def setup_run(quintpar, extrapar, techpar, sess_data, comp='surp', 
              gab_ori='all'):
    """
    setup_run(quintpar, extrapar, techpar, sess_data)
    
    Sets up run(s) by setting seed, getting classes and number of ROIs.

    Required args:
        - quintpar (QuintPar)  : named tuple containing quintile parameters
        - extrapar (dict)      : dictionary with extra parameters
            ['seed'] (int)    : seed to use
            ['shuffle'] (bool): if analysis is on shuffled data
            ['uniqueid'] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ['compdir'] (str) : specific output comparison directory
            ['device'] (str)   : device to use (e.g., 'cpu' or 'cuda')
            ['fontdir'] (str)  : directory in which additional fonts are 
                                 located
            ['output'] (str)   : main output directiory
            ['plt_bkend'] (str): plt backend to use (e.g., 'agg' or None)
            ['reseed'] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
            - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_surps (list)    : list of lists of number of surprise sequences
                                  (doubled if 'half' comparison), 
                                  structured as datasets x class 

    Optional args:
        - comp (str)           : comparison
                                 default: 'surp'
        - gab_ori (str or list): Gabor orientations,
                                 for comp values like DoriE, DoriA, etc.
                                 default: 'shared'

    Returns:
        - extrapar (dict)   : dictionary with extra parameters
            ['classes'] (list): list of class names
            ['n_rois'] (int)  : number of ROIs
            ['seed'] (int)    : seed to use
            ['shuffle'] (bool): if analysis is on shuffled data
            ['uniqueid'] (str): unique ID for the analysis
        - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                              (1 or 2 if an additional test set is included), 
                              each structured as sequences x frames x ROIs
        - seq_classes (list): list of 2D arrays of sequence classes
                              (1 or 2 if an additional test set is included), 
                              each structured as class values x 1
        - n_surps (list)    : list of lists of number of surprise sequences
                              (doubled if 'half' comparison),
                              structured as datasets x class

    """

    extrapar = copy.deepcopy(extrapar)
    if techpar['reseed']: # reset seed         
        extrapar['seed'] = None
    extrapar['seed'] = gen_util.seed_all(extrapar['seed'], techpar['device'])
    extrapar['classes'] = get_classes(comp, gab_ori)
    
    [roi_seqs, seq_classes, n_surps] = copy.deepcopy(sess_data)
    extrapar['n_rois']  = roi_seqs[0].shape[-1]

    return extrapar, roi_seqs, seq_classes, n_surps


#############################################
def all_runs_sk(n_runs, analyspar, logregpar, quintpar, sesspar, stimpar, 
                 extrapar, techpar, sess_data):
    """
    all_runs_sk(n_runs, analyspar, logregpar, quintpar, sesspar, stimpar, 
                 extrapar, techpar, sess_data)

    Does all runs of a logistic regression on the specified comparison
    and session data using sklearn. Records hyperparameters, all models,  
    tr_stats dictionary. Plots scores and training statistics. 
    
    Required args:
        - n_runs (int)         : number of runs to do
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - quintpar (QuintPar)  : named tuple containing quintile parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ['seed'] (int)    : seed to use
            ['shuffle'] (bool): if analysis is on shuffled data
            ['uniqueid'] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ['compdir'] (str) : specific output comparison directory
            ['device'] (str)   : device to use (e.g., 'cpu' or 'cuda')
            ['fontdir'] (str)  : directory in which additional fonts are 
                                 located
            ['output'] (str)   : main output directiory
            ['plt_bkend'] (str): plt backend to use (e.g., 'agg' or None)
            ['reseed'] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
            - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_surps (list)    : list of lists of number of surprise sequences
                                  (doubled if 'half' comparison), 
                                  structured as datasets x class
    """
    logregpar = sess_ntuple_util.get_modif_ntuple(
        logregpar, ['batchsize', 'lr', 'wd'], [None, None, None])
    if techpar['device'] == 'gpu':
        print('sklearn method not implemented with GPU.')

    [extrapar, roi_seqs, seq_classes, n_surps] = setup_run(
         quintpar, extrapar, techpar, sess_data, logregpar.comp, 
         gab_ori=stimpar.gab_ori)
    main_data = [roi_seqs[0], seq_classes[0]]

    samples = [False for _ in n_surps]
    if logregpar.ctrl:
        # get n_surp for last class
        samples = [n_surp[-1] for n_surp in n_surps]
        keep_all = [val in logregpar.comp for val in ['ori', 'dir_reg', 'half']]
        if sum(keep_all) > 0: # get n_surp for all classes
            samples = n_surps

    plot_util.manage_mpl(techpar['plt_bkend'], fontdir=techpar['fontdir'])

    extrapar = copy.deepcopy(extrapar)
    extrapar['n_runs'] = n_runs
    rundir = get_rundir(extrapar['n_runs'], extrapar['uniqueid'], logregpar.alg)
    extrapar['dirname'] = file_util.createdir([techpar['output'], 
        techpar['compdir'], rundir])
    
    scale_str = sess_str_util.scale_par_str(analyspar.scale, 'print')
    shuff_str = sess_str_util.shuff_par_str(extrapar['shuffle'], 'labels')
    print(f'\nRuns ({n_runs}): {scale_str}{shuff_str}')

    split_test = False

    returns = logreg_util.run_logreg_cv_sk(
        main_data[0], main_data[1], logregpar._asdict(), extrapar, 
        analyspar.scale, samples[0], split_test, extrapar['seed'], 
        techpar['parallel'], catch_set_prob=True)
    if returns is None:
        return
    else:
        mod_cvs, cv, extrapar = returns


    hyperpars = save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    # Additional data for scoring
    set_names  = ['train', 'test']
    extra_data, extra_cv = None, None
    extra_name = sess_str_util.ext_test_str(
        logregpar.q1v4, logregpar.regvsurp, logregpar.comp)
    if cv._split_test:
        set_names.append('test_out')
    if len(roi_seqs) == 2:
        extra_data = [roi_seqs[1], seq_classes[1]]
        extra_cv = logreg_util.StratifiedShuffleSplitMod(
            n_splits=cv.n_splits, train_p=0.5, sample=samples[1], 
            bal=logregpar.bal) # since train_p cannot be 1.0
        if extra_name is None:
            raise ValueError('Extra test dataset not labelled.')
        set_names.append(extra_name)

    mod_cvs = logreg_util.test_logreg_cv_sk(
        mod_cvs, cv, extrapar['scoring'], main_data=main_data, 
        extra_data=extra_data, extra_name=extra_name, extra_cv=extra_cv, 
        catch_set_prob=True)
    if mod_cvs is None:
        return

    # Get and save best model
    best_mod_idx = np.argmax(mod_cvs[f'{set_names[-1]}_neg_log_loss'])
    best_mod     = mod_cvs['estimator'][best_mod_idx]
    # Save data used for best model
    plot_data, plot_targ, data_names = [], [], []
    for name, subcv, data in zip(
        ['train', extra_name], [cv, extra_cv], [main_data, extra_data]):
        if data is None:
            continue
        if name == 'train':
            idx = subcv._set_idx[best_mod_idx][0]
        else:
            idx = [i for sub in subcv._set_idx[best_mod_idx] for i in sub]
        plot_data.append(data[0][idx])
        plot_targ.append(data[1][idx])
        data_names.append(name)

    tr_stats = save_tr_stats(
        plot_data, plot_targ, data_names, analyspar, stimpar, 
        extrapar['n_rois'], logregpar.alg, best_mod, 
        dirname=extrapar['dirname'])
   
    # Get scores dataframe
    scores = logreg_util.create_score_df_sk(
        mod_cvs, best_mod_idx, set_names, extrapar['scoring'])
    info = info_dict(
        analyspar, sesspar, stimpar, extrapar, logregpar.comp, logregpar.alg, 
        n_rois=extrapar['n_rois'])
    full_scores = save_scores(
        info, scores, key_order=info_dict(), dirname=extrapar['dirname'])

    # Save best model
    logreg_plots.plot_traces_scores(
        hyperpars, tr_stats, full_scores, plot_wei=best_mod_idx)

    plt.close('all')



#############################################
def single_run_pt(run_n, analyspar, logregpar, quintpar, sesspar, stimpar, 
                  extrapar, techpar, sess_data):
    """
    single_run_pt(run_n, analyspar, logregpar, quintpar, sesspar, stimpar, 
                  extrapar, techpar, sess_data)

    Does a single run of a logistic regression using PyTorch on the specified 
    comparison and session data. Records hyperparameters, best model, last 
    model, tr_stats dictionary. Plots scores and training statistics. 
    
    Required args:
        - run_n (int)          : run number
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - quintpar (QuintPar)  : named tuple containing quintile parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ['seed'] (int)    : seed to use
            ['shuffle'] (bool): if analysis is on shuffled data
            ['uniqueid'] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ['compdir'] (str) : specific output comparison directory
            ['device'] (str)   : device to use (e.g., 'cpu' or 'cuda')
            ['fontdir'] (str)  : directory in which additional fonts are 
                                 located
            ['output'] (str)   : main output directiory
            ['plt_bkend'] (str): plt backend to use (e.g., 'agg' or None)
            ['reseed'] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)  : list of 3D arrays of selected ROI trace seqs
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
           - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_surps (list)    : number of surprise sequences, listed by 
                                  quintile, (doubled if 'half' comparison)
    """
    
    extrapar = copy.deepcopy(extrapar)
    extrapar['seed'] *= run_n + 1 # ensure different seed for each run

    [extrapar, roi_seqs, seq_classes, n_surps] = setup_run(
        quintpar, extrapar, techpar, sess_data, logregpar.comp, 
        gab_ori=stimpar.gab_ori)

    extrapar['run_n'] = run_n
    scale_str = sess_str_util.scale_par_str(analyspar.scale, 'print')
    shuff_str = sess_str_util.shuff_par_str(extrapar['shuffle'], 'labels')
    run_n = extrapar['run_n']
    print(f'\nRun: {run_n}{scale_str}{shuff_str}')

    for i in range(len(roi_seqs)):
        if logregpar.ctrl: # select a random subsample
            roi_seqs[i], seq_classes[i] = sample_seqs(
                roi_seqs[i], seq_classes[i], n_surps[i][-1])  
        if logregpar.bal: # balance classes
            roi_seqs[i], seq_classes[i] = data_util.bal_classes(
                roi_seqs[i], seq_classes[i])

    plot_util.manage_mpl(techpar['plt_bkend'], fontdir=techpar['fontdir'])
    
    thresh_cl = 2
    if sesspar.runtype == 'pilot':
        thresh_cl = 1

    rundir = get_rundir(extrapar['run_n'], extrapar['uniqueid'], logregpar.alg)
    extrapar['dirname'] = file_util.createdir(
        [techpar['output'], techpar['compdir'], rundir])

    returns = init_logreg_model_pt(
        roi_seqs, seq_classes, logregpar, extrapar, analyspar.scale, 
        techpar['device'], thresh_cl=thresh_cl, catch_set_prob=True)
    if returns is None:
        return
    else:
        mod, dls, extrapar = returns

    hyperpars = save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    data_names = ['train']
    extra_name = sess_str_util.ext_test_str(
        logregpar.q1v4, logregpar.regvsurp, logregpar.comp)
    idx = [0]
    if len(roi_seqs) == 2:
        idx.append(-1)
        if extra_name == '':
            raise ValueError('Extra test dataset not labelled.')
        data_names.append(extra_name)
    plot_data = [dls[i].dataset.data for i in idx]
    plot_targ = [dls[i].dataset.targets for i in idx]

    tr_stats = save_tr_stats(
        plot_data, plot_targ, data_names, analyspar, stimpar, 
        extrapar['n_rois'], alg=logregpar.alg, dirname=extrapar['dirname'])

    info = info_dict(
        analyspar, sesspar, stimpar, extrapar, logregpar.comp, logregpar.alg, 
        n_rois=tr_stats['n_rois'])
    scores = logreg_util.fit_model_pt(
        info, logregpar.n_epochs, mod, dls, techpar['device'], 
        extrapar['dirname'], ep_freq=techpar['ep_freq'], 
        test_dl2_name=extra_name)

    print('Run {}: training done.\n'.format(extrapar['run_n']))

    # save scores in dataframe
    full_scores = save_scores(
        info, scores, key_order=info_dict(), dirname=extrapar['dirname'])

    # plot traces and scores (only for first 5 runs)
    # no plotting for the rest to reduce number of files generated
    if run_n <= 5:
        logreg_plots.plot_traces_scores(
            hyperpars, tr_stats, full_scores, plot_wei=True)

    plt.close('all')


#############################################
def run_regr(sess, analyspar, stimpar, logregpar, quintpar, extrapar, techpar):
    """
    Does runs of a logistic regressions on the specified comparison on a 
    session.

    Required args:
        - sess (Session)       : Session on which to run logistic regression
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 analysis parameters
        - quintpar (QuintPar)  : named tuple containing quintile analysis 
                                 parameters
        - extrapar (dict)      : dictionary containing additional analysis 
                                 parameters
            ['uniqueid'] (str or int): unique ID for analysis
            ['seed'] (int)           : seed to seed random processes with
        - techpar (dict)       : dictionary containing technical analysi 
                                 parameters
            ['device'] (str)   : device name (i.e., 'cuda' or 'cpu')
            ['ep_freq'] (int)  : frequency at which to print loss to console
            ['fontdir'] (str)  : directory in which additional fonts are 
                                 located
            ['n_reg'] (int)    : number of regular runs
            ['n_shuff'] (int)  : number of shuffled runs
            ['output'] (str)   : general directory in which to save output
            ['parallel'] (bool): if True, runs are done in parallel
            ['plt_bkend'] (str): pyplot backend to use
            ['reseed'] (bool)  : if True, each run is reseeded
    """

    techpar = copy.deepcopy(techpar)
    sesspar = sess_ntuple_util.init_sesspar(
        sess.sess_n, False, sess.plane,sess.line, runtype=sess.runtype, 
        mouse_n=sess.mouse_n)

    class_var, surps = get_class_pars(logregpar.comp, stimpar.stimtype)
    split_oris = sess_str_util.get_split_oris(logregpar.comp)

    try:
        sess_data = get_sess_data(
            sess, analyspar, stimpar, quintpar, class_var, surps, 
            regvsurp=logregpar.regvsurp, split_oris=split_oris)
    except ValueError as err:
        catch_phr = ['fit these criteria', 'No frames']
        catch = sum(phr in str(err) for phr in catch_phr)
        if catch:            
            print(err)
            return
        else:
            raise err

    for n_runs, shuffle in zip(
        [techpar['n_reg'], techpar['n_shuff']], [False, True]):
        if n_runs == 0:
            continue
        extrapar = copy.deepcopy(extrapar)
        extrapar['shuffle'] = shuffle
        techpar = copy.deepcopy(techpar)
        techpar['compdir'] = sess_gen_util.get_analysdir(
            sesspar.mouse_n, sesspar.sess_n, sesspar.plane, analyspar.fluor, 
            analyspar.scale, stimpar.stimtype, stimpar.bri_dir, 
            stimpar.bri_size, stimpar.gabk, logregpar.comp, logregpar.ctrl, 
            extrapar['shuffle'])
        if logregpar.alg == 'pytorch':
            techpar['compdir'] = '{}_pt'.format(techpar['compdir'])
            if techpar['parallel'] and n_runs != 0:
                n_jobs = gen_util.get_n_jobs(n_runs)
                Parallel(n_jobs=n_jobs)(delayed(single_run_pt)
                    (run, analyspar, logregpar, quintpar, sesspar, stimpar, 
                    extrapar, techpar, sess_data) for run in range(n_runs))
            elif n_runs != 0:
                for run in range(n_runs):
                    single_run_pt(run, analyspar, logregpar, quintpar, sesspar, 
                        stimpar, extrapar, techpar, sess_data)
        elif logregpar.alg == 'sklearn':
            all_runs_sk(n_runs, analyspar, logregpar, quintpar, sesspar, 
                stimpar, extrapar, techpar, sess_data)
        else:
            gen_util.accepted_values_error('logregpar.alg', logregpar.alg, 
                ['pytorch', 'sklearn'])


#############################################
def collate_scores(direc, all_labels, alg='sklearn'):
    """
    collate_scores(direc, all_labels)

    Collects the analysis information and scores from the last epoch recorded 
    for a run and returns in dataframe.
    
    Required args:
        - direc (str)      : path to the specific comparison run folder
        - all_labels (list): ordered list of columns to save to dataframe

    Optional args:
        - alg (str): algorithm used to run logistic regression 
                     ('sklearn' or 'pytorch')
                     default: 'sklearn'

    Return:
        - scores (pd DataFrame): Dataframe containing run analysis information
                                 and scores from the last epoch recorded.
    """

    print(direc)
 
    scores = pd.DataFrame(columns=all_labels)

    ep_info, hyperpars = logreg_util.get_scores(direc, alg)
    if ep_info is None:
        comp_dict = get_compdir_dict(direc, no_lists=True)
        comp_dict['scale'] = hyperpars['analyspar']['scale']
        comp_dict['runtype'] = hyperpars['sesspar']['runtype']
        comp_dict['line'] = hyperpars['sesspar']['line']
        comp_dict['n_rois'] = hyperpars['extrapar']['n_rois']
        for col in all_labels: # ensures correct order
            if col in comp_dict.keys():
                scores.loc[0, col] = comp_dict[col]
    else:
        for col in all_labels:
            if col in ep_info.columns:
                if alg == 'pytorch':
                    scores.loc[0, col] = ep_info[col].item()
                elif alg == 'sklearn':
                    scores[col] = ep_info[col]

    return scores


#############################################
def remove_overlap_comp_dir(gen_dirs, stimtype='gabors', comp='surp'):
    """
    remove_overlap_comp_dir(gen_dirs)

    Returns list of directories with those corresponding to comparisons with 
    overlapping names removed.

    Required args:
        - gen_dirs (list): list of directories

    Optional args:
        - stimtype (str) : stimulus type
                           default: 'gabors'
        - comp (str)     : type of comparison
                           default: 'surp'
    """

    all_comps = get_comps(stimtype)

    for other_comp in all_comps:
        if comp != other_comp and comp in other_comp:
            gen_dirs = [gen_dir for gen_dir in gen_dirs 
                if other_comp not in gen_dir]

    return gen_dirs


#############################################
def run_collate(output, stimtype='gabors', comp='surp', ctrl=False, 
                alg='sklearn', parallel=False):
    """
    run_collate(output)

    Collects the analysis information and scores from the last epochs recorded 
    for all runs for a comparison type, and saves to a dataframe.

    Overwrites any existing dataframe of collated data.  
    
    Required args:
        - output (str): general directory in which summary dataframe is saved
    
    Optional args:
        - stimtype (str) : stimulus type
                           default: 'gabors'
        - comp (str)     : type of comparison
                           default: 'surp'
        - ctrl (bool)    : if True, control comparisons are analysed
                           default: False
        - alg (str)      : algorithm used to run logistic regression 
                           ('sklearn' or 'pytorch')
                           default: 'sklearn'
        - parallel (bool): if True, run information is collected in parallel
                           default: False

    Returns:
        - all_scores (pd DataFrame): dataframe compiling all the scores for 
                                     logistic regression runs in the output 
                                     folder that correspond to the stimulus 
                                     type and comparison type criteria
    """

    if not os.path.isdir(output):
        print(f'{output} does not exist.')
        return

    ext_test = sess_str_util.ext_test_str(
        ('q1v4' in output), ('rvs' in output), comp)
    if ext_test == '':
        ext_test = None

    ctrl_str = sess_str_util.ctrl_par_str(ctrl)
    gen_dirs = file_util.getfiles(
        output, 'subdirs', [stimtype[0:3], comp, ctrl_str])
                         
    if alg == 'sklearn':
        gen_dirs = [gen_dir for gen_dir in gen_dirs if '_pt' not in gen_dir]
    elif alg == 'pytorch':
        gen_dirs = [gen_dir for gen_dir in gen_dirs if '_pt' in gen_dir]
    else:
        gen_util.accepted_values_error('alg', alg, ['sklearn', 'pytorch'])

    gen_dirs = remove_overlap_comp_dir(gen_dirs, stimtype, comp)
    if not ctrl:
        gen_dirs = [gen_dir for gen_dir in gen_dirs 
            if 'ctrl' not in gen_dir]

    if len(gen_dirs) == 0:
        print('No runs found.')
        return

    run_dirs = [run_dir for gen_dir in gen_dirs
        for run_dir in file_util.getfiles(gen_dir, 'subdirs')]

    # identify, remove and flag empty directories     
    empty_dirs = [run_dir for run_dir in run_dirs 
        if len(os.listdir(run_dir)) == 0]

    if len(empty_dirs) != 0:
        print('EMPTY DIRECTORIES:')
        for empty_dir in empty_dirs:
            run_dirs.remove(empty_dir)
            print(f'    {empty_dir}')

    all_labels = info_dict() + \
        logreg_util.get_sc_labs(True, ext_test_name=ext_test) + ['saved']

    if parallel:
        n_jobs = gen_util.get_n_jobs(len(run_dirs))
        scores_list = Parallel(n_jobs=n_jobs)(delayed(collate_scores)
            (run_dir, all_labels, alg) for run_dir in run_dirs)
        if len(scores_list) != 0:
            all_scores = pd.concat(scores_list)
            all_scores = all_scores[all_labels] # reorder
        else:
            all_scores = pd.DataFrame(columns=all_labels)
    else:
        all_scores = pd.DataFrame(columns=all_labels)
        for run_dir in run_dirs:
            scores = collate_scores(run_dir, all_labels, alg)
            all_scores = all_scores.append(scores)

    # sort df by mouse, session, plane, line, fluor, scale, shuffle, stimtype,
    # comp, uniqueid, run_n, runtype
    sorter = info_dict()[0:13]
    
    all_scores = all_scores.sort_values(by=sorter).reset_index(drop=True)

    savename = get_df_name('collate', stimtype, comp, ctrl, alg)
    file_util.saveinfo(all_scores, savename, output, overwrite=True)

    return all_scores


#############################################
def calc_stats(scores_summ, curr_lines, curr_idx, CI=0.95, ext_test=None, 
               stats='mean', shuffle=False):
    """
    calc_stats(scores_summ, curr_lines, curr_idx)

    Calculates statistics on scores from runs with specific analysis criteria
    and records them in the summary scores dataframe.  
    
    Required args:
        - scores_summ (pd DataFrame): DataFrame containing scores summary
        - curr_lines (pd DataFrame) : DataFrame lines corresponding to specific
                                      analysis criteria
        - curr_idx (int)            : Current row in the scores summary 
                                      DataFrame 
    
    Optional args:
        - CI (num)        : Confidence interval around which to collect 
                            percentile values
                            default: 0.95
        - extra_test (str): Name of extra test set, if any (None if none)
                            default: None
        - stats (str)     : stats to take, i.e., 'mean' or 'median'
                            default: 'mean'
        - shuffle (bool)  : If True, data is for shuffled, and will be averaged 
                            across runs before taking stats
                            default: False

    Returns:
        - scores_summ (pd DataFrame): Updated DataFrame containing scores, as
                                      well as epoch_n, runs_total, runs_nan 
                                      summaries
    """

    scores_summ = copy.deepcopy(scores_summ)

    # score labels to perform statistics on
    sc_labs = ['epoch_n'] + logreg_util.get_sc_labs(
        True, ext_test_name=ext_test)

    # avoids accidental nuisance dropping by pandas
    curr_lines['epoch_n'] = curr_lines['epoch_n'].astype(float)

    if shuffle: # group runs and take mean or median across
        scores_summ.loc[curr_idx, 'mouse_n'] = -1
        keep_lines = \
            [col for col in curr_lines.columns if col in sc_labs] + ['run_n']
        grped_lines = curr_lines[keep_lines].groupby('run_n', as_index=False)
        if stats == 'mean':
            curr_lines = grped_lines.mean() # automatically skips NaNs
        elif stats == 'median':
            curr_lines = grped_lines.median() # automatically skips NaNs
        else:
            gen_util.accepted_values_error('stats', stats, ['mean', 'median'])

    # calculate n_runs (without nans and with)
    scores_summ.loc[curr_idx, 'runs_total'] = len(curr_lines)
    scores_summ.loc[curr_idx, 'runs_nan'] = curr_lines['epoch_n'].isna().sum()


    # percentiles to record
    ps, p_names = math_util.get_percentiles(CI)

    for sc_lab in sc_labs:
        if sc_lab in curr_lines.keys():
            cols = []
            vals = []
            data = curr_lines[sc_lab].astype(float)
            for stat in ['mean', 'median']:
                cols.extend([stat])
                vals.extend(
                    [math_util.mean_med(data, stats=stat, nanpol='omit')])
            for error in ['std', 'sem']:
                cols.extend([error])
                vals.extend([math_util.error_stat(
                    data, stats='mean', error=error, nanpol='omit')])
            # get 25th and 75th quartiles
            cols.extend(['q25', 'q75'])
            vals.extend(math_util.error_stat(
                data, stats='median', error='std', nanpol='omit'))                                            
            # get other percentiles (for CI)
            cols.extend(p_names)
            vals.extend(math_util.error_stat(
                data, stats='median', error='std', nanpol='omit', qu=ps))
            
            # get MAD
            cols.extend(['mad'])
            vals.extend([math_util.error_stat(
                data, stats='median', error='sem', nanpol='omit')])

            # plug in values
            cols = [f'{sc_lab}_{name}' for name in cols]
            gen_util.set_df_vals(scores_summ, curr_idx, cols, vals)

    return scores_summ


#############################################
def run_analysis(output, stimtype='gabors', comp='surp', ctrl=False, 
                 CI=0.95, alg='sklearn', parallel=False):  
    """
    run_analysis(output)

    Calculates statistics on scores from runs for each specific analysis 
    criteria and saves them in the summary scores dataframe.

    Overwrites any existing dataframe of analysed data.

    Required args:
        - output (str): general directory in which summary dataframe is saved
    
    Optional args:
        - stimtype (str) : stimulus type
                           default: 'gabors'
        - comp (str)     : type of comparison
                           default: 'surp'
        - ctrl (bool)    : if True, control comparisons are analysed
                           default: False
        - CI (num)       : CI for shuffled data
                           default: 0.95
        - alg (str)      : algorithm used to run logistic regression 
                           ('sklearn' or 'pytorch')
                           default: 'sklearn'
        - parallel (bool): if True, run information is collected in parallel
                           default: False
    """

    all_scores_df = run_collate(output, stimtype, comp, ctrl, alg, parallel)

    stats = 'mean' # across runs for shuffle CIs

    if all_scores_df is None:
        return

    scores_summ = pd.DataFrame()

    ext_test = sess_str_util.ext_test_str(
        ('q1v4' in output), ('rvs' in output), comp)
    if ext_test == '':
        ext_test = None

    # common labels
    comm_labs = gen_util.remove_if(info_dict(), 
        ['uniqueid', 'run_n', 'epoch_n'])

    # get all unique comb of labels
    for acr_shuff in [False, True]:
        if not acr_shuff:
            df_unique = all_scores_df[comm_labs].drop_duplicates()
        else:
            df_unique = all_scores_df[gen_util.remove_if(comm_labs, 
                ['mouse_n', 'n_rois'])].drop_duplicates()
        for _, df_row in df_unique.iterrows():
            if acr_shuff and not df_row['shuffle']:
                # second pass, only shuffle
                continue
            vals = [df_row[x] for x in comm_labs]
            curr_lines = gen_util.get_df_vals(all_scores_df, comm_labs, vals)
            # assign values to current line in summary df
            curr_idx = len(scores_summ)
            gen_util.set_df_vals(scores_summ, curr_idx, comm_labs, vals)
            # calculate stats
            scores_summ = calc_stats(scores_summ, curr_lines, curr_idx, CI, 
                ext_test, stats=stats, shuffle=acr_shuff)

    savename = get_df_name('analyse', stimtype, comp, ctrl, alg)
    
    file_util.saveinfo(scores_summ, savename, output, overwrite=True)


#############################################    
def run_plot(output, stimtype='gabors', comp='surp', ctrl=False, bri_dir='both', 
             fluor='dff', scale=True, CI=0.95, alg='sklearn', plt_bkend=None, 
             fontdir=None, modif=False):
    """
    run_plot(output)

    Plots summary data for a specific comparison, for each datatype in a 
    separate figure and saves figures. 

    Required args:
        - output (str): general directory in which summary dataframe is saved
    
    Optional args:
        - stimtype (str) : stimulus type
                           default: 'gabors'
        - comp (str)     : type of comparison
                           default: 'surp'
        - ctrl (bool)    : if True, control comparisons are analysed
                           default: False
        - bri_dir (str)  : brick direction
                           default: 'both'
        - fluor (str)    : fluorescence trace type
                           default: 'dff'
        - scale (bool)   : whether data is scaled by ROI
                           default: True
        - CI (num)       : CI for shuffled data
                           default: 0.95
        - alg (str)      : algorithm used to run logistic regression 
                           ('sklearn' or 'pytorch')
                           default: 'sklearn'
        - plt_bkend (str): pyplot backend to use
                           default: None
        - fontdir (str)  : directory in which additional fonts are located
                           default: None
        - modif (bool)   : if True, plots are made in a modified (simplified 
                           way)
                           default: False
    """

    if comp in ['half_right', 'half_left']:
        bri_dir = '{}'.format(comp.replace('half_', ''))

    savename = get_df_name('analyse', stimtype, comp, ctrl, alg)

    logreg_plots.plot_summ(output, savename, stimtype, comp, ctrl, bri_dir, 
        fluor, scale, CI, plt_bkend, fontdir, modif)

